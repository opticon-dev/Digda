import openai
import base64
import json
from PIL import Image
import os
import replicate
import requests
import time
import shutil
from datetime import datetime
import io
from typing import Union, Tuple

# =============================================================================
# ê¸°ë³¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================


def encode_image_to_base64(image_path):
    """ì´ë¯¸ì§€ íŒŒì¼ì„ Base64 ë¬¸ìì—´ë¡œ ë³€í™˜"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def image_buffer_to_base64(image_buffer):
    return base64.b64encode(image_buffer).decode("utf-8")


def pil_to_filelike(pil_img, format="PNG"):
    buf = io.BytesIO()
    pil_img.save(buf, format=format)
    buf.seek(0)
    return buf


def open_image_by_type(image_path: Union[str, bytes, io.BytesIO]):
    if isinstance(image_path, str):
        # íŒŒì¼ ê²½ë¡œ
        _img = None
        with Image.open(image_path) as img:
            _img = img
        return _img

    elif isinstance(image_path, bytes):
        # raw bytes
        img = Image.open(io.BytesIO(image_path))
        return img
    elif isinstance(image_path, io.BytesIO):
        # ì´ë¯¸ BytesIO ê°ì²´
        img = Image.open(image_path)
        return img
    else:
        raise NotImplementedError


class ImageProcessor:
    def __init__(self, OPENAI_API_KEY, REPLICATE_API_TOKEN):
        self.open_ai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
        self.replicate_client = replicate.Client(api_token=REPLICATE_API_TOKEN)

    def process_1(self, image):
        # ë‹¨ê³„ 1: ê°€êµ¬ ì¸ì‹ ë° í¬ë¡­
        print("\nğŸ”¥ [ë‹¨ê³„ 1] ê°€êµ¬ ì¸ì‹ ë° í¬ë¡­ ì‹œì‘...")
        step1_result = FurnitureCropper(self.open_ai_client).process(image)

        if not step1_result:
            print("âŒ ë‹¨ê³„ 1 ì‹¤íŒ¨: ê°€êµ¬ ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None

        print(
            f"âœ… ë‹¨ê³„ 1 ì™„ë£Œ: {len(step1_result['detected_furniture'])}ê°œ ê°€êµ¬ ì²˜ë¦¬ë¨"
        )
        # ë‹¨ê³„ 2: ë°°ê²½ ì œê±°
        print("\nğŸ”¥ [ë‹¨ê³„ 2] Bria ë°°ê²½ ì œê±° ì‹œì‘...")
        step2_result = BackgroundRemover(self.replicate_client).process(
            step1_result.get("cropped_images")
        )
        return step2_result

    def process_2(self, selected_images):
        print("\nğŸ”¥ [ë‹¨ê³„ 3] HunYuan3D 3D ë³€í™˜ ì‹œì‘...")
        processed_files = ImgToModeling(self.replicate_client).process(selected_images)

        if not processed_files:
            print("âŒ ë‹¨ê³„ 3 ì‹¤íŒ¨: 3D ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return None

        return processed_files


# =============================================================================
# ë‹¨ê³„ 1: GPT APIë¥¼ ì‚¬ìš©í•œ ê°€êµ¬ ì¸ì‹ ë° ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­ (ì¤‘ë³µ ì œê±° ë²„ì „)
# =============================================================================


def calculate_box_area(box):
    """ë°•ìŠ¤ì˜ ë©´ì  ê³„ì‚°"""
    x1, y1, x2, y2 = box
    return (x2 - x1) * (y2 - y1)


def calculate_overlap_ratio(box1, box2):
    """ë‘ ë°•ìŠ¤ì˜ ê²¹ì¹˜ëŠ” ë¹„ìœ¨ ê³„ì‚° (ì‘ì€ ë°•ìŠ¤ ê¸°ì¤€)"""
    x1_1, y1_1, x2_1, y2_1 = box1
    x1_2, y1_2, x2_2, y2_2 = box2

    # ê²¹ì¹˜ëŠ” ì˜ì—­ ê³„ì‚°
    overlap_x1 = max(x1_1, x1_2)
    overlap_y1 = max(y1_1, y1_2)
    overlap_x2 = min(x2_1, x2_2)
    overlap_y2 = min(y2_1, y2_2)

    # ê²¹ì¹˜ëŠ” ì˜ì—­ì´ ìˆëŠ”ì§€ í™•ì¸
    if overlap_x1 >= overlap_x2 or overlap_y1 >= overlap_y2:
        return 0.0

    # ê²¹ì¹˜ëŠ” ë©´ì 
    overlap_area = (overlap_x2 - overlap_x1) * (overlap_y2 - overlap_y1)

    # ë” ì‘ì€ ë°•ìŠ¤ì˜ ë©´ì 
    area1 = calculate_box_area(box1)
    area2 = calculate_box_area(box2)
    smaller_area = min(area1, area2)

    # ì‘ì€ ë°•ìŠ¤ ê¸°ì¤€ ê²¹ì¹˜ëŠ” ë¹„ìœ¨
    return overlap_area / smaller_area if smaller_area > 0 else 0.0


class FurnitureRebuilder:
    GPT_MODEL = "gpt-4o"

    def __init__(self, open_ai_client):
        self.open_ai_client = open_ai_client

    def write_prompt(self, img_width, img_height):
        # GPT í”„ë¡¬í”„íŠ¸ (ì£¼ìš” ê°€êµ¬ ì¤‘ì‹¬)
        return f"""
ë‹¹ì‹ ì€ ì¸í…Œë¦¬ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ ê±°ì‹¤ ì‚¬ì§„ì—ì„œ ì£¼ìš” ê°€êµ¬ë“¤ì„ ì •í™•íˆ ì°¾ê³  ê·¸ ê°€êµ¬ê°€ ì¼ë¶€ë§Œ ë³´ì´ë”ë¼ë„ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì„ ìƒìƒí•´ì„œ ì™„ì„±í•˜ì„¸ìš”.

ì´ë¯¸ì§€ í¬ê¸°: {img_width} x {img_height} í”½ì…€

ìš°ì„ ìˆœìœ„ë³„ ê°€êµ¬ ëª©ë¡:

ğŸ  ëŒ€í˜• ê°€êµ¬ (ìµœìš°ì„ ):
- ì†ŒíŒŒ, ì‡¼íŒŒ (sofa, couch, sectional)
- í° í…Œì´ë¸” (dining table, large coffee table)
- í° ì„ ë°˜/ì±…ì¥ (bookshelf, large cabinet)
- ì¹¨ëŒ€ (bed, mattress)

ğŸª‘ ì¤‘í˜• ê°€êµ¬:
- ì˜ì (chair, armchair, recliner)
- ì‘ì€ í…Œì´ë¸” (side table, coffee table)
- TV/ëª¨ë‹ˆí„° (television, monitor)
- ì‚¬ë‹¤ë¦¬ (ladder, step)

ğŸ§¸ ì†Œí˜• ê°€êµ¬/ì†Œí’ˆ:
- ì¡°ëª… (lamp, floor lamp)
- ì‹ë¬¼/í™”ë¶„ (plant, pot)
- ì¥ì‹í’ˆ (decoration, vase)
- ì¿ ì…˜ (cushion, pillow)

ì¤‘ìš” ì§€ì¹¨:
1. í° ê°€êµ¬ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¸ì‹
2. ê° ê°€êµ¬ëŠ” ì¶©ë¶„íˆ í° í¬ê¸°ì—¬ì•¼ í•¨ (ìµœì†Œ 50x50 í”½ì…€)
3. ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” ê°œë³„ ê°€êµ¬ë§Œ í¬í•¨
4. ì• ë§¤í•˜ê±°ë‚˜ ì¼ë¶€ë§Œ ë³´ì´ëŠ” ê²ƒì€ ì œì™¸

ì´ë¯¸ì§€ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
ì„¤ëª… ì—†ì´ ë°˜í™˜í•˜ì„¸ìš”.
        """

    def process(self, image_path):
        """ë‹¨ê³„ 1: ì¤‘ë³µ ì œê±°ëœ ê°€êµ¬ ì¸ì‹ ë° ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­"""
        print("=" * 70)
        print("ğŸš€ ë‹¨ê³„ 1: GPT ê°€êµ¬ ì¸ì‹ + ì¤‘ë³µ ì œê±° + ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­")
        print("=" * 70)

        # 1. ê°€êµ¬ ì¸ì‹ (ì¤‘ë³µ ì œê±° í¬í•¨)
        print("\nğŸ“ 1ë‹¨ê³„: ê°€êµ¬ ì¸ì‹ ë° ì¤‘ë³µ ì œê±°")
        furniture_list = self._detect_furniture_with_gpt_filtered(image_path)

    def _detect_furniture_with_gpt_filtered(self, image_path):
        """GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€êµ¬ë¥¼ ì¸ì‹í•˜ê³  ì¤‘ë³µ ì œê±°"""
        print(f"ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {image_path}")

        if isinstance(image_path, str):
            # íŒŒì¼ ê²½ë¡œ
            with Image.open(image_path) as img:
                img_width, img_height = img.width, img.height
            base64_image = encode_image_to_base64(image_path)

        elif isinstance(image_path, bytes):
            # raw bytes
            img = Image.open(io.BytesIO(image_path))
            img_width, img_height = img.width, img.height
            base64_image = image_buffer_to_base64(image_path)

        elif isinstance(image_path, io.BytesIO):
            # ì´ë¯¸ BytesIO ê°ì²´
            img = Image.open(image_path)
            img_width, img_height = img.width, img.height
            # BytesIO â†’ bytes ë³€í™˜í•´ì„œ base64 ì¸ì½”ë”©
            base64_image = image_buffer_to_base64(image_path.getvalue())

        else:
            raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…: {type(image_path)}")

        # GPT API í˜¸ì¶œ
        response = self.open_ai_client.chat.completions.create(
            model=self.GPT_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": self.write_prompt(img_width, img_height),
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                                "detail": "high",
                            },
                        },
                    ],
                }
            ],
            max_tokens=3000,
            temperature=0.1,
        )

        # ì‘ë‹µ ì²˜ë¦¬
        response_text = response.choices[0].message.content.strip()
        print(f"ğŸ“ GPT ì‘ë‹µ ë°›ìŒ (ê¸¸ì´: {len(response_text)}ì)")

        # JSON ì¶”ì¶œ
        json_str = response_text.replace("```json", "").replace("```", "").strip()
        print(json_str)

    def _filter_overlapping_furniture(self, furniture_list, overlap_threshold=0.7):
        """ì¤‘ë³µë˜ëŠ” ê°€êµ¬ ì œê±° (í° ê°€êµ¬ ìš°ì„ , ê²¹ì¹˜ëŠ” ë¹„ìœ¨ì´ ë†’ì€ ì‘ì€ ê°€êµ¬ ì œê±°)"""
        print(f"\nğŸ” ì¤‘ë³µ ê°€êµ¬ í•„í„°ë§ ì‹œì‘ (ì„ê³„ê°’: {overlap_threshold*100}%)")

        # ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (í° ê°€êµ¬ë¶€í„°)
        sorted_furniture = sorted(
            furniture_list, key=lambda x: calculate_box_area(x["box"]), reverse=True
        )

        filtered_furniture = []
        removed_count = 0

        for current_furniture in sorted_furniture:
            current_box = current_furniture["box"]
            current_area = calculate_box_area(current_box)
            current_name = current_furniture["name"]

            is_overlapping = False

            # ì´ë¯¸ ì„ íƒëœ ê°€êµ¬ë“¤ê³¼ ë¹„êµ
            for selected_furniture in filtered_furniture:
                selected_box = selected_furniture["box"]
                selected_name = selected_furniture["name"]

                overlap_ratio = calculate_overlap_ratio(current_box, selected_box)

                if overlap_ratio > overlap_threshold:
                    print(
                        f"âŒ {current_name} (ë©´ì : {current_area}) - {selected_name}ì™€ {overlap_ratio:.1%} ê²¹ì¹¨"
                    )
                    is_overlapping = True
                    removed_count += 1
                    break

            if not is_overlapping:
                filtered_furniture.append(current_furniture)
                print(f"âœ… {current_name} (ë©´ì : {current_area}) - ìœ ì§€")

        print(
            f"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {len(furniture_list)}ê°œ â†’ {len(filtered_furniture)}ê°œ (ì œê±°: {removed_count}ê°œ)"
        )
        return filtered_furniture

    def _categorize_furniture_by_size(self, furniture_list, img_width, img_height):
        """ê°€êµ¬ë¥¼ í¬ê¸°ë³„ë¡œ ë¶„ë¥˜"""
        total_image_area = img_width * img_height

        large_furniture = []  # í° ê°€êµ¬ (ì´ë¯¸ì§€ì˜ 3% ì´ìƒ)
        medium_furniture = []  # ì¤‘ê°„ ê°€êµ¬ (ì´ë¯¸ì§€ì˜ 1-3%)
        small_furniture = []  # ì‘ì€ ê°€êµ¬ (ì´ë¯¸ì§€ì˜ 1% ë¯¸ë§Œ)

        for furniture in furniture_list:
            area = calculate_box_area(furniture["box"])
            area_ratio = area / total_image_area

            furniture["area"] = area
            furniture["area_ratio"] = area_ratio

            if area_ratio >= 0.03:  # 3% ì´ìƒ
                large_furniture.append(furniture)
            elif area_ratio >= 0.01:  # 1-3%
                medium_furniture.append(furniture)
            else:  # 1% ë¯¸ë§Œ
                small_furniture.append(furniture)

        print(f"ğŸ“ í¬ê¸°ë³„ ë¶„ë¥˜:")
        print(f"   ğŸ  í° ê°€êµ¬ ({len(large_furniture)}ê°œ): ì´ë¯¸ì§€ì˜ 3% ì´ìƒ")
        print(f"   ğŸª‘ ì¤‘ê°„ ê°€êµ¬ ({len(medium_furniture)}ê°œ): ì´ë¯¸ì§€ì˜ 1-3%")
        print(f"   ğŸ§¸ ì‘ì€ ê°€êµ¬ ({len(small_furniture)}ê°œ): ì´ë¯¸ì§€ì˜ 1% ë¯¸ë§Œ")

        return large_furniture, medium_furniture, small_furniture

    def calculate_size_analysis(self, furniture_list, img_width, img_height):
        """ìƒëŒ€ì  í¬ê¸° ë¶„ì„ ê³„ì‚°"""
        if not furniture_list:
            return {}

        print("\nğŸ“Š ìƒëŒ€ì  í¬ê¸° ë¶„ì„ ì¤‘...")

        # ê¸°ë³¸ ì •ë³´
        total_image_area = img_width * img_height
        largest_furniture_area = max(
            furniture.get("area", 0) for furniture in furniture_list
        )

        # ê° ê°€êµ¬ì˜ ìƒëŒ€ì  í¬ê¸° ì •ë³´ ê³„ì‚°
        furniture_size_comparison = []

        for furniture in furniture_list:
            name = furniture["name"]
            area = furniture.get("area", 0)

            # ë¹„ìœ¨ ê³„ì‚°
            area_ratio_to_image = (area / total_image_area) * 100
            area_ratio_to_largest = (
                (area / largest_furniture_area) * 100
                if largest_furniture_area > 0
                else 0
            )

            # í¬ê¸° ë“±ê¸‰ ê²°ì •
            if area_ratio_to_image >= 3.0:
                size_rank = "ëŒ€í˜•"
            elif area_ratio_to_image >= 1.0:
                size_rank = "ì¤‘í˜•"
            else:
                size_rank = "ì†Œí˜•"

            # ìƒëŒ€ ì ìˆ˜ (ê°€ì¥ í° ê°€êµ¬ = 100ì )
            relative_size_score = round(area_ratio_to_largest, 1)

            size_info = {
                "name": name,
                "absolute_area": area,
                "area_ratio_to_image": f"{area_ratio_to_image:.2f}%",
                "area_ratio_to_largest": f"{area_ratio_to_largest:.1f}%",
                "size_rank": size_rank,
                "relative_size_score": relative_size_score,
            }

            furniture_size_comparison.append(size_info)
            print(
                f"  ğŸ“ {name:15s}: {area:,}pxÂ² ({area_ratio_to_image:.2f}%, {size_rank}, ì ìˆ˜: {relative_size_score})"
            )

        # í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬
        furniture_size_comparison.sort(key=lambda x: x["absolute_area"], reverse=True)

        size_analysis = {
            "image_dimensions": {"width": img_width, "height": img_height},
            "total_image_area": total_image_area,
            "largest_furniture_area": largest_furniture_area,
            "furniture_count": len(furniture_list),
            "size_distribution": {
                "large": len(
                    [
                        f
                        for f in furniture_list
                        if (f.get("area", 0) / total_image_area) >= 0.03
                    ]
                ),
                "medium": len(
                    [
                        f
                        for f in furniture_list
                        if 0.01 <= (f.get("area", 0) / total_image_area) < 0.03
                    ]
                ),
                "small": len(
                    [
                        f
                        for f in furniture_list
                        if (f.get("area", 0) / total_image_area) < 0.01
                    ]
                ),
            },
            "furniture_size_comparison": furniture_size_comparison,
        }

        print(
            f"âœ… í¬ê¸° ë¶„ì„ ì™„ë£Œ: ëŒ€í˜• {size_analysis['size_distribution']['large']}ê°œ, ì¤‘í˜• {size_analysis['size_distribution']['medium']}ê°œ, ì†Œí˜• {size_analysis['size_distribution']['small']}ê°œ"
        )

        return size_analysis

    def crop_furniture_centered_filtered(
        self, image_path, furniture_list, output_dir="furniture_crops_filtered"
    ):
        """ê° ê°€êµ¬ë¥¼ ì¤‘ì‹¬ì— ë§ì¶°ì„œ í¬ë¡­ (í•„í„°ë§ëœ ë²„ì „)"""
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # ì´ë¯¸ì§€ ì—´ê¸°
        img = open_image_by_type(image_path)
        img_width, img_height = img.size
        if isinstance(img, str):
            base_name = os.path.splitext(os.path.basename(image_path))[0]
        else:
            base_name = "IMG_FROM_RHINO"
        print(f"ğŸ¯ {len(furniture_list)}ê°œ ê°€êµ¬ë¥¼ ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­í•©ë‹ˆë‹¤...")

        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬ (ë©´ì  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
        sorted_furniture = sorted(
            furniture_list, key=lambda x: x.get("area", 0), reverse=True
        )

        cropped_images = []

        for i, furniture in enumerate(sorted_furniture):
            name = furniture["name"]
            category = furniture.get("category", "medium")
            priority = furniture.get("priority", "medium")
            x1, y1, x2, y2 = furniture["box"]
            area = furniture.get("area", 0)

            try:
                # ì›ë³¸ ê°€êµ¬ í¬ê¸°
                furniture_width = x2 - x1
                furniture_height = y2 - y1
                furniture_center_x = (x1 + x2) // 2
                furniture_center_y = (y1 + y2) // 2

                # í¬ë¡­ í¬ê¸° ê²°ì • (ì¹´í…Œê³ ë¦¬ë³„ ì—¬ë°± ì¡°ì •)
                if category == "large":
                    margin_ratio = 0.05  # í° ê°€êµ¬ëŠ” 5% ì—¬ë°±
                elif category == "medium":
                    margin_ratio = 0.1  # ì¤‘ê°„ ê°€êµ¬ëŠ” 10% ì—¬ë°±
                else:
                    margin_ratio = 0.15  # ì‘ì€ ê°€êµ¬ëŠ” 15% ì—¬ë°±

                margin_x = int(furniture_width * margin_ratio)
                margin_y = int(furniture_height * margin_ratio)
                crop_width = furniture_width + 2 * margin_x
                crop_height = furniture_height + 2 * margin_y

                # ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­ ì¢Œí‘œ ê³„ì‚°
                crop_x1 = max(0, furniture_center_x - crop_width // 2)
                crop_y1 = max(0, furniture_center_y - crop_height // 2)
                crop_x2 = min(img_width, crop_x1 + crop_width)
                crop_y2 = min(img_height, crop_y1 + crop_height)

                # ê²½ê³„ ì¡°ì •
                if crop_x2 - crop_x1 < crop_width:
                    crop_x1 = max(0, crop_x2 - crop_width)
                if crop_y2 - crop_y1 < crop_height:
                    crop_y1 = max(0, crop_y2 - crop_height)

                # ì´ë¯¸ì§€ í¬ë¡­
                cropped_img = img.crop((crop_x1, crop_y1, crop_x2, crop_y2))

                # íŒŒì¼ëª… ì •ë¦¬ (ìš°ì„ ìˆœìœ„ í¬í•¨)
                safe_name = name.replace(" ", "_").replace("/", "_").replace("\\", "_")
                filename = f"{base_name}_{i+1:02d}_{priority}_{safe_name}.png"
                filepath = os.path.join(output_dir, filename)

                # ì €ì¥
                cropped_img.save(filepath)

                # ê²°ê³¼ ì •ë³´ ì €ì¥
                crop_info = {
                    "original_furniture": furniture,
                    "crop_coordinates": [crop_x1, crop_y1, crop_x2, crop_y2],
                    "crop_size": f"{crop_x2-crop_x1}x{crop_y2-crop_y1}",
                    "filename": filename,
                    "filepath": filepath,
                }
                cropped_images.append(crop_info)

                area_ratio = area / (img_width * img_height) * 100
                print(
                    f"âœ… {i+1:2d}. {name:15s} ({priority:6s}) â†’ {filename} ({crop_x2-crop_x1}x{crop_y2-crop_y1}, {area_ratio:.1f}%)"
                )

            except Exception as e:
                print(f"âŒ {name} í¬ë¡­ ì‹¤íŒ¨: {e}")

        print(f"ğŸ‰ ì´ {len(cropped_images)}ê°œ ê°€êµ¬ í¬ë¡­ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}/")

        return cropped_images


class FurnitureCropper:
    GPT_MODEL = "gpt-4o"

    def __init__(self, open_ai_client):
        self.open_ai_client = open_ai_client

    def write_prompt(self, img_width, img_height):
        # GPT í”„ë¡¬í”„íŠ¸ (ì£¼ìš” ê°€êµ¬ ì¤‘ì‹¬)
        return f"""
ë‹¹ì‹ ì€ ì¸í…Œë¦¬ì–´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ ê±°ì‹¤ ì‚¬ì§„ì—ì„œ ì£¼ìš” ê°€êµ¬ë“¤ì„ ì •í™•íˆ ì°¾ì•„ì£¼ì„¸ìš”.

ì´ë¯¸ì§€ í¬ê¸°: {img_width} x {img_height} í”½ì…€

ìš°ì„ ìˆœìœ„ë³„ ê°€êµ¬ ëª©ë¡:

ğŸ  ëŒ€í˜• ê°€êµ¬ (ìµœìš°ì„ ):
- ì†ŒíŒŒ, ì‡¼íŒŒ (sofa, couch, sectional)
- í° í…Œì´ë¸” (dining table, large coffee table)
- í° ì„ ë°˜/ì±…ì¥ (bookshelf, large cabinet)
- ì¹¨ëŒ€ (bed, mattress)

ğŸª‘ ì¤‘í˜• ê°€êµ¬:
- ì˜ì (chair, armchair, recliner)
- ì‘ì€ í…Œì´ë¸” (side table, coffee table)
- TV/ëª¨ë‹ˆí„° (television, monitor)
- ì‚¬ë‹¤ë¦¬ (ladder, step)

ğŸ§¸ ì†Œí˜• ê°€êµ¬/ì†Œí’ˆ:
- ì¡°ëª… (lamp, floor lamp)
- ì‹ë¬¼/í™”ë¶„ (plant, pot)
- ì¥ì‹í’ˆ (decoration, vase)
- ì¿ ì…˜ (cushion, pillow)

ì¤‘ìš” ì§€ì¹¨:
1. í° ê°€êµ¬ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¸ì‹
2. ê° ê°€êµ¬ëŠ” ì¶©ë¶„íˆ í° í¬ê¸°ì—¬ì•¼ í•¨ (ìµœì†Œ 50x50 í”½ì…€)
3. ëª…í™•í•˜ê²Œ êµ¬ë¶„ë˜ëŠ” ê°œë³„ ê°€êµ¬ë§Œ í¬í•¨
4. ì• ë§¤í•˜ê±°ë‚˜ ì¼ë¶€ë§Œ ë³´ì´ëŠ” ê²ƒì€ ì œì™¸

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
"furniture_list": [
    {{
    "name": "êµ¬ì²´ì ì¸_ê°€êµ¬_ì´ë¦„",
    "category": "large/medium/small", 
    "priority": "high/medium/low",
    "box": [x1, y1, x2, y2],
    "confidence": "high/medium/low"
    }}
]
}}

ì¢Œí‘œ ê·œì¹™:
- [x1, y1] = ì™¼ìª½ ì•„ë˜ ëª¨ì„œë¦¬
- [x2, y2] = ì˜¤ë¥¸ìª½ ìœ„ ëª¨ì„œë¦¬
- 0 â‰¤ x1 < x2 â‰¤ {img_width}
- 0 â‰¤ y1 < y2 â‰¤ {img_height}

ì„¤ëª… ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
        """

    def process(self, image_path):
        """ë‹¨ê³„ 1: ì¤‘ë³µ ì œê±°ëœ ê°€êµ¬ ì¸ì‹ ë° ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­"""
        print("=" * 70)
        print("ğŸš€ ë‹¨ê³„ 1: GPT ê°€êµ¬ ì¸ì‹ + ì¤‘ë³µ ì œê±° + ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­")
        print("=" * 70)

        # 1. ê°€êµ¬ ì¸ì‹ (ì¤‘ë³µ ì œê±° í¬í•¨)
        print("\nğŸ“ 1ë‹¨ê³„: ê°€êµ¬ ì¸ì‹ ë° ì¤‘ë³µ ì œê±°")
        furniture_list = self._detect_furniture_with_gpt_filtered(image_path)

        if not furniture_list:
            print("âŒ ê°€êµ¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return None

        # 2. ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­ ë° ì´ë¯¸ì§€ ì €ì¥
        print("\nğŸ“ 2ë‹¨ê³„: ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­")
        cropped_images = self.crop_furniture_centered_filtered(
            image_path, furniture_list
        )

        # 3. ìƒëŒ€ì  í¬ê¸° ë¶„ì„ ì¶”ê°€
        print("\nğŸ“ 3ë‹¨ê³„: ìƒëŒ€ì  í¬ê¸° ë¶„ì„")
        with Image.open(image_path) as img:
            img_width, img_height = img.size

        size_analysis = self.calculate_size_analysis(
            furniture_list, img_width, img_height
        )

        # 4. ê²°ê³¼ ì €ì¥ (í¬ê¸° ë¶„ì„ í¬í•¨)
        result_data = {
            "detected_furniture": furniture_list,
            "cropped_images": cropped_images,
            "size_analysis": size_analysis,  # ìƒˆë¡œ ì¶”ê°€ëœ í¬ê¸° ë¶„ì„ ì •ë³´
            "summary": {
                "total_detected": len(furniture_list),
                "successfully_cropped": len(cropped_images),
                "large_furniture": len(
                    [f for f in furniture_list if f.get("category") == "large"]
                ),
                "medium_furniture": len(
                    [f for f in furniture_list if f.get("category") == "medium"]
                ),
                "small_furniture": len(
                    [f for f in furniture_list if f.get("category") == "small"]
                ),
            },
        }

        print("\n" + "=" * 70)
        print(f"âœ… ë‹¨ê³„ 1 ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ì„ íƒ ê°€êµ¬: {len(furniture_list)}ê°œ")
        print(f"ğŸ–¼ï¸  í¬ë¡­ëœ ì´ë¯¸ì§€: {len(cropped_images)}ê°œ")
        print(f"ğŸ“ ìƒëŒ€ì  í¬ê¸° ë¶„ì„: í¬í•¨ë¨")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: step1_furniture_detection_filtered.json")
        print("=" * 70)

        return result_data

    def _detect_furniture_with_gpt_filtered(self, image_path):
        """GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€êµ¬ë¥¼ ì¸ì‹í•˜ê³  ì¤‘ë³µ ì œê±°"""
        print(f"ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {image_path}")

        if isinstance(image_path, str):
            # íŒŒì¼ ê²½ë¡œ
            with Image.open(image_path) as img:
                img_width, img_height = img.width, img.height
            base64_image = encode_image_to_base64(image_path)

        elif isinstance(image_path, bytes):
            # raw bytes
            img = Image.open(io.BytesIO(image_path))
            img_width, img_height = img.width, img.height
            base64_image = image_buffer_to_base64(image_path)

        elif isinstance(image_path, io.BytesIO):
            # ì´ë¯¸ BytesIO ê°ì²´
            img = Image.open(image_path)
            img_width, img_height = img.width, img.height
            # BytesIO â†’ bytes ë³€í™˜í•´ì„œ base64 ì¸ì½”ë”©
            base64_image = image_buffer_to_base64(image_path.getvalue())

        else:
            raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…: {type(image_path)}")

        # GPT API í˜¸ì¶œ
        response = self.open_ai_client.chat.completions.create(
            model=self.GPT_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": self.write_prompt(img_width, img_height),
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                                "detail": "high",
                            },
                        },
                    ],
                }
            ],
            max_tokens=3000,
            temperature=0.1,
        )

        # ì‘ë‹µ ì²˜ë¦¬
        response_text = response.choices[0].message.content.strip()
        print(f"ğŸ“ GPT ì‘ë‹µ ë°›ìŒ (ê¸¸ì´: {len(response_text)}ì)")

        # JSON ì¶”ì¶œ
        json_str = response_text.replace("```json", "").replace("```", "").strip()

        try:
            data = json.loads(json_str)
            furniture_list = data.get("furniture_list", [])

            print(f"âœ… ì´ {len(furniture_list)}ê°œ ê°€êµ¬ ë°œê²¬")

            # ì¢Œí‘œ ê²€ì¦ ë° ì •ë¦¬
            valid_furniture = []
            for item in furniture_list:
                name = item.get("name", "unknown")
                category = item.get("category", "medium")
                priority = item.get("priority", "medium")
                box = item.get("box", [])
                confidence = item.get("confidence", "medium")

                if len(box) == 4:
                    x1, y1, x2, y2 = box

                    # ì¢Œí‘œ ë²”ìœ„ í™•ì¸ ë° ìˆ˜ì •
                    x1 = max(0, min(x1, img_width - 1))
                    x2 = max(x1 + 10, min(x2, img_width))
                    y1 = max(0, min(y1, img_height - 1))
                    y2 = max(y1 + 10, min(y2, img_height))

                    # ìµœì†Œ í¬ê¸° í™•ì¸ (50x50 í”½ì…€ ì´ìƒ)
                    if (x2 - x1) >= 50 and (y2 - y1) >= 50:
                        area = (x2 - x1) * (y2 - y1)
                        valid_furniture.append(
                            {
                                "name": name,
                                "category": category,
                                "priority": priority,
                                "box": [x1, y1, x2, y2],
                                "confidence": confidence,
                                "area": area,
                                "size": f"{x2-x1}x{y2-y1}",
                            }
                        )
                        print(
                            f"âœ… {name} ({category}/{priority}): [{x1},{y1},{x2},{y2}] - {x2-x1}x{y2-y1}"
                        )
                    else:
                        print(f"âš ï¸  {name}: ë„ˆë¬´ ì‘ìŒ ({x2-x1}x{y2-y1})")
                else:
                    print(f"âŒ {name}: ì¢Œí‘œ í˜•ì‹ ì˜¤ë¥˜")

            # í¬ê¸°ë³„ ë¶„ë¥˜
            large_furniture, medium_furniture, small_furniture = (
                self._categorize_furniture_by_size(
                    valid_furniture, img_width, img_height
                )
            )

            # ì¤‘ë³µ ì œê±° (í° ê°€êµ¬ë¶€í„° ìš°ì„ )
            all_furniture = large_furniture + medium_furniture + small_furniture
            filtered_furniture = self._filter_overlapping_furniture(
                all_furniture, overlap_threshold=0.6
            )

            print(f"ğŸ“‹ ìµœì¢… ì„ íƒëœ ê°€êµ¬: {len(filtered_furniture)}ê°œ")
            return filtered_furniture

        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì‘ë‹µ ë‚´ìš©: {response_text[:500]}...")
            return []

    def _filter_overlapping_furniture(self, furniture_list, overlap_threshold=0.7):
        """ì¤‘ë³µë˜ëŠ” ê°€êµ¬ ì œê±° (í° ê°€êµ¬ ìš°ì„ , ê²¹ì¹˜ëŠ” ë¹„ìœ¨ì´ ë†’ì€ ì‘ì€ ê°€êµ¬ ì œê±°)"""
        print(f"\nğŸ” ì¤‘ë³µ ê°€êµ¬ í•„í„°ë§ ì‹œì‘ (ì„ê³„ê°’: {overlap_threshold*100}%)")

        # ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (í° ê°€êµ¬ë¶€í„°)
        sorted_furniture = sorted(
            furniture_list, key=lambda x: calculate_box_area(x["box"]), reverse=True
        )

        filtered_furniture = []
        removed_count = 0

        for current_furniture in sorted_furniture:
            current_box = current_furniture["box"]
            current_area = calculate_box_area(current_box)
            current_name = current_furniture["name"]

            is_overlapping = False

            # ì´ë¯¸ ì„ íƒëœ ê°€êµ¬ë“¤ê³¼ ë¹„êµ
            for selected_furniture in filtered_furniture:
                selected_box = selected_furniture["box"]
                selected_name = selected_furniture["name"]

                overlap_ratio = calculate_overlap_ratio(current_box, selected_box)

                if overlap_ratio > overlap_threshold:
                    print(
                        f"âŒ {current_name} (ë©´ì : {current_area}) - {selected_name}ì™€ {overlap_ratio:.1%} ê²¹ì¹¨"
                    )
                    is_overlapping = True
                    removed_count += 1
                    break

            if not is_overlapping:
                filtered_furniture.append(current_furniture)
                print(f"âœ… {current_name} (ë©´ì : {current_area}) - ìœ ì§€")

        print(
            f"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {len(furniture_list)}ê°œ â†’ {len(filtered_furniture)}ê°œ (ì œê±°: {removed_count}ê°œ)"
        )
        return filtered_furniture

    def _categorize_furniture_by_size(self, furniture_list, img_width, img_height):
        """ê°€êµ¬ë¥¼ í¬ê¸°ë³„ë¡œ ë¶„ë¥˜"""
        total_image_area = img_width * img_height

        large_furniture = []  # í° ê°€êµ¬ (ì´ë¯¸ì§€ì˜ 3% ì´ìƒ)
        medium_furniture = []  # ì¤‘ê°„ ê°€êµ¬ (ì´ë¯¸ì§€ì˜ 1-3%)
        small_furniture = []  # ì‘ì€ ê°€êµ¬ (ì´ë¯¸ì§€ì˜ 1% ë¯¸ë§Œ)

        for furniture in furniture_list:
            area = calculate_box_area(furniture["box"])
            area_ratio = area / total_image_area

            furniture["area"] = area
            furniture["area_ratio"] = area_ratio

            if area_ratio >= 0.03:  # 3% ì´ìƒ
                large_furniture.append(furniture)
            elif area_ratio >= 0.01:  # 1-3%
                medium_furniture.append(furniture)
            else:  # 1% ë¯¸ë§Œ
                small_furniture.append(furniture)

        print(f"ğŸ“ í¬ê¸°ë³„ ë¶„ë¥˜:")
        print(f"   ğŸ  í° ê°€êµ¬ ({len(large_furniture)}ê°œ): ì´ë¯¸ì§€ì˜ 3% ì´ìƒ")
        print(f"   ğŸª‘ ì¤‘ê°„ ê°€êµ¬ ({len(medium_furniture)}ê°œ): ì´ë¯¸ì§€ì˜ 1-3%")
        print(f"   ğŸ§¸ ì‘ì€ ê°€êµ¬ ({len(small_furniture)}ê°œ): ì´ë¯¸ì§€ì˜ 1% ë¯¸ë§Œ")

        return large_furniture, medium_furniture, small_furniture

    def calculate_size_analysis(self, furniture_list, img_width, img_height):
        """ìƒëŒ€ì  í¬ê¸° ë¶„ì„ ê³„ì‚°"""
        if not furniture_list:
            return {}

        print("\nğŸ“Š ìƒëŒ€ì  í¬ê¸° ë¶„ì„ ì¤‘...")

        # ê¸°ë³¸ ì •ë³´
        total_image_area = img_width * img_height
        largest_furniture_area = max(
            furniture.get("area", 0) for furniture in furniture_list
        )

        # ê° ê°€êµ¬ì˜ ìƒëŒ€ì  í¬ê¸° ì •ë³´ ê³„ì‚°
        furniture_size_comparison = []

        for furniture in furniture_list:
            name = furniture["name"]
            area = furniture.get("area", 0)

            # ë¹„ìœ¨ ê³„ì‚°
            area_ratio_to_image = (area / total_image_area) * 100
            area_ratio_to_largest = (
                (area / largest_furniture_area) * 100
                if largest_furniture_area > 0
                else 0
            )

            # í¬ê¸° ë“±ê¸‰ ê²°ì •
            if area_ratio_to_image >= 3.0:
                size_rank = "ëŒ€í˜•"
            elif area_ratio_to_image >= 1.0:
                size_rank = "ì¤‘í˜•"
            else:
                size_rank = "ì†Œí˜•"

            # ìƒëŒ€ ì ìˆ˜ (ê°€ì¥ í° ê°€êµ¬ = 100ì )
            relative_size_score = round(area_ratio_to_largest, 1)

            size_info = {
                "name": name,
                "absolute_area": area,
                "area_ratio_to_image": f"{area_ratio_to_image:.2f}%",
                "area_ratio_to_largest": f"{area_ratio_to_largest:.1f}%",
                "size_rank": size_rank,
                "relative_size_score": relative_size_score,
            }

            furniture_size_comparison.append(size_info)
            print(
                f"  ğŸ“ {name:15s}: {area:,}pxÂ² ({area_ratio_to_image:.2f}%, {size_rank}, ì ìˆ˜: {relative_size_score})"
            )

        # í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬
        furniture_size_comparison.sort(key=lambda x: x["absolute_area"], reverse=True)

        size_analysis = {
            "image_dimensions": {"width": img_width, "height": img_height},
            "total_image_area": total_image_area,
            "largest_furniture_area": largest_furniture_area,
            "furniture_count": len(furniture_list),
            "size_distribution": {
                "large": len(
                    [
                        f
                        for f in furniture_list
                        if (f.get("area", 0) / total_image_area) >= 0.03
                    ]
                ),
                "medium": len(
                    [
                        f
                        for f in furniture_list
                        if 0.01 <= (f.get("area", 0) / total_image_area) < 0.03
                    ]
                ),
                "small": len(
                    [
                        f
                        for f in furniture_list
                        if (f.get("area", 0) / total_image_area) < 0.01
                    ]
                ),
            },
            "furniture_size_comparison": furniture_size_comparison,
        }

        print(
            f"âœ… í¬ê¸° ë¶„ì„ ì™„ë£Œ: ëŒ€í˜• {size_analysis['size_distribution']['large']}ê°œ, ì¤‘í˜• {size_analysis['size_distribution']['medium']}ê°œ, ì†Œí˜• {size_analysis['size_distribution']['small']}ê°œ"
        )

        return size_analysis

    def crop_furniture_centered_filtered(
        self, image_path, furniture_list, output_dir="furniture_crops_filtered"
    ):
        """ê° ê°€êµ¬ë¥¼ ì¤‘ì‹¬ì— ë§ì¶°ì„œ í¬ë¡­ (í•„í„°ë§ëœ ë²„ì „)"""
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # ì´ë¯¸ì§€ ì—´ê¸°
        img = open_image_by_type(image_path)
        img_width, img_height = img.size
        if isinstance(img, str):
            base_name = os.path.splitext(os.path.basename(image_path))[0]
        else:
            base_name = "IMG_FROM_RHINO"
        print(f"ğŸ¯ {len(furniture_list)}ê°œ ê°€êµ¬ë¥¼ ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­í•©ë‹ˆë‹¤...")

        # ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë ¬ (ë©´ì  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ)
        sorted_furniture = sorted(
            furniture_list, key=lambda x: x.get("area", 0), reverse=True
        )

        cropped_images = []

        for i, furniture in enumerate(sorted_furniture):
            name = furniture["name"]
            category = furniture.get("category", "medium")
            priority = furniture.get("priority", "medium")
            x1, y1, x2, y2 = furniture["box"]
            area = furniture.get("area", 0)

            try:
                # ì›ë³¸ ê°€êµ¬ í¬ê¸°
                furniture_width = x2 - x1
                furniture_height = y2 - y1
                furniture_center_x = (x1 + x2) // 2
                furniture_center_y = (y1 + y2) // 2

                # í¬ë¡­ í¬ê¸° ê²°ì • (ì¹´í…Œê³ ë¦¬ë³„ ì—¬ë°± ì¡°ì •)
                if category == "large":
                    margin_ratio = 0.05  # í° ê°€êµ¬ëŠ” 5% ì—¬ë°±
                elif category == "medium":
                    margin_ratio = 0.1  # ì¤‘ê°„ ê°€êµ¬ëŠ” 10% ì—¬ë°±
                else:
                    margin_ratio = 0.15  # ì‘ì€ ê°€êµ¬ëŠ” 15% ì—¬ë°±

                margin_x = int(furniture_width * margin_ratio)
                margin_y = int(furniture_height * margin_ratio)
                crop_width = furniture_width + 2 * margin_x
                crop_height = furniture_height + 2 * margin_y

                # ì¤‘ì‹¬ ë§ì¶¤ í¬ë¡­ ì¢Œí‘œ ê³„ì‚°
                crop_x1 = max(0, furniture_center_x - crop_width // 2)
                crop_y1 = max(0, furniture_center_y - crop_height // 2)
                crop_x2 = min(img_width, crop_x1 + crop_width)
                crop_y2 = min(img_height, crop_y1 + crop_height)

                # ê²½ê³„ ì¡°ì •
                if crop_x2 - crop_x1 < crop_width:
                    crop_x1 = max(0, crop_x2 - crop_width)
                if crop_y2 - crop_y1 < crop_height:
                    crop_y1 = max(0, crop_y2 - crop_height)

                # ì´ë¯¸ì§€ í¬ë¡­
                cropped_img = img.crop((crop_x1, crop_y1, crop_x2, crop_y2))

                # íŒŒì¼ëª… ì •ë¦¬ (ìš°ì„ ìˆœìœ„ í¬í•¨)
                safe_name = name.replace(" ", "_").replace("/", "_").replace("\\", "_")
                filename = f"{base_name}_{i+1:02d}_{priority}_{safe_name}.png"
                filepath = os.path.join(output_dir, filename)

                # ì €ì¥
                cropped_img.save(filepath)

                # # ê²°ê³¼ ì •ë³´ ì €ì¥
                # crop_info = {
                #     "original_furniture": furniture,
                #     "crop_coordinates": [crop_x1, crop_y1, crop_x2, crop_y2],
                #     "crop_size": f"{crop_x2-crop_x1}x{crop_y2-crop_y1}",
                #     "filename": filename,
                #     "filepath": filepath,
                # }
                cropped_images.append(cropped_img)

                area_ratio = area / (img_width * img_height) * 100
                print(
                    f"âœ… {i+1:2d}. {name:15s} ({priority:6s}) â†’ {filename} ({crop_x2-crop_x1}x{crop_y2-crop_y1}, {area_ratio:.1f}%)"
                )

            except Exception as e:
                print(f"âŒ {name} í¬ë¡­ ì‹¤íŒ¨: {e}")

        print(f"ğŸ‰ ì´ {len(cropped_images)}ê°œ ê°€êµ¬ í¬ë¡­ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}/")

        return cropped_images


class BackgroundRemover:
    # =============================================================================
    # ë‹¨ê³„ 2: Bria Remove Background ëª¨ë¸ ì‚¬ìš©
    # =============================================================================
    def __init__(self, replicate_client):
        self.replicate_client = replicate_client

    def process(
        self,
        cropped_files,
    ):
        """Bria ëª¨ë¸ì„ ì‚¬ìš©í•œ ë°°ê²½ ì œê±° ì¼ê´„ ì²˜ë¦¬"""

        print("=" * 60)
        print(f"ğŸ­ Bria ë°°ê²½ ì œê±° ì‘ì—… ì‹œì‘")
        print(f"ğŸ“Š ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(cropped_files)}ê°œ")
        print(f"ğŸ¤– ëª¨ë¸: bria/remove-background")
        print("=" * 60)

        processed_files = []
        success_count = 0

        for i, cropped_file in enumerate(cropped_files, 1):

            # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±

            print(f"\nğŸ”„ [{i}/{len(cropped_files)}] ì²˜ë¦¬ ì¤‘")

            # Bria ë°°ê²½ ì œê±° ì‹œë„
            success, background_removed_file_byte = self.remove_background_per_file(
                cropped_file
            )

            if success:
                success_count += 1
                processed_files.append(background_removed_file_byte)

            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ íŒŒì¼ ë³µì‚¬
                print(f"   âš ï¸  ë°°ê²½ ì œê±° ì‹¤íŒ¨")

            # API í˜¸ì¶œ ê°„ ì ì‹œ ëŒ€ê¸° (Rate Limit ë°©ì§€)
            if i < len(cropped_files):
                print("   â³ 2ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(2)

        print("\n" + "=" * 60)
        print(f"ğŸ‰ Bria ë°°ê²½ ì œê±° ì‘ì—… ì™„ë£Œ!")
        print(f"âœ… ì„±ê³µ: {success_count}/{len(cropped_files)}ê°œ")
        print("=" * 60)

        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: step2_background_removal_bria.json")

        return processed_files

    def remove_background_per_file(self, cropped_image) -> Tuple[bool, bytes]:
        """Bria remove-background ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°°ê²½ ì œê±°"""
        print(f"ğŸ­ Bria ë°°ê²½ ì œê±° ì‹œì‘: ")

        cropped_image = pil_to_filelike(cropped_image)
        # Bria ëª¨ë¸ ì‹¤í–‰
        output = self.replicate_client.run(
            "bria/remove-background",
            input={
                "image": cropped_image,
                "content_moderation": False,
                "preserve_partial_alpha": True,
            },
        )

        print(f"ğŸ” Bria ì‘ë‹µ íƒ€ì…: {type(output)}")

        # outputì„ ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
        image_bytes = output.read()

        # PILë¡œ ë¡œë“œí•´ì„œ ë‚¨ì€ í”½ì…€ ìˆ˜ í™•ì¸
        img = Image.open(io.BytesIO(image_bytes)).convert("RGBA")
        alpha = img.split()[-1]  # ì•ŒíŒŒ ì±„ë„
        non_zero_pixels = sum(alpha.getdata()) // 255  # 0~255 ê°’ â†’ í”½ì…€ ìˆ˜ë¡œ ë³€í™˜

        print(f"   ğŸŸ¢ ë‚¨ì€ í”½ì…€ ìˆ˜: {non_zero_pixels}")

        if non_zero_pixels < 30:
            print(f"   âš ï¸ ë‚¨ì€ í”½ì…€ì´ {non_zero_pixels}ê°œ â†’ None ë¦¬í„´")
            return False, None

        print(f"âœ… Bria ë°°ê²½ ì œê±° ì™„ë£Œ")
        return True, image_bytes


class ImgToModeling:
    def __init__(self, replicate_client):
        self.replicate_client = replicate_client

    # =============================================================================
    # ë‹¨ê³„ 3: HunYuan3D ëª¨ë¸ì„ ì‚¬ìš©í•œ 3D ë³€í™˜
    # =============================================================================
    def run_hunyuan3d(self, image, output_filename):
        """HunYuan3D ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°€êµ¬ ì´ë¯¸ì§€ë¥¼ 3D ëª¨ë¸ë¡œ ë³€í™˜"""
        try:
            print(f"ğŸ¨ 3D ë³€í™˜ ì‹œì‘:")

            input_data = {
                "image": image,
                "remove_background": False,  # ì´ë¯¸ ë°°ê²½ì´ ì œê±°ë¨
            }

            # HunYuan3D ëª¨ë¸ ì‹¤í–‰
            output = self.replicate_client.run(
                "ndreca/hunyuan3d-2:0602bae6db1ce420f2690339bf2feb47e18c0c722a1f02e9db9abd774abaff5d",
                input=input_data,
            )

            print(f"ğŸ” HunYuan3D ì‘ë‹µ íƒ€ì…: {type(output)}")

            # 3D ë©”ì‹œ ë‹¤ìš´ë¡œë“œ
            mesh_url = output["mesh"]
            response = requests.get(mesh_url)

            with open(output_filename, "wb") as file:
                file.write(response.content)

            print(f"âœ… 3D ëª¨ë¸ ìƒì„± ì™„ë£Œ: {output_filename}")
            return True

        except Exception as e:
            print(f"âŒ 3D ë³€í™˜ ì‹¤íŒ¨ : {e}")
            return False

    def process(self, selected_images, output_dir="furniture_3d_models"):
        """Bria ë°°ê²½ ì œê±°ëœ ê°€êµ¬ë“¤ì„ 3D ëª¨ë¸ë¡œ ë³€í™˜"""
        try:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)

            print("=" * 60)
            print(f"ğŸ¨ HunYuan3D 3D ë³€í™˜ ì‘ì—… ì‹œì‘")
            print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
            print(f"ğŸ“Š ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜: {len(selected_images)}ê°œ")
            print(f"ğŸ¤– ëª¨ë¸: ndreca/hunyuan3d-2")
            print("=" * 60)

            processed_files = []
            success_count = 0
            time_str = datetime.now().strftime("%Y%m%d_%H%M%S")

            for i, selected_image in enumerate(selected_images, 1):

                output_filename = f"3d_{i}_{time_str}.glb"
                output_path = os.path.join(output_dir, output_filename)

                print(f"\nğŸ”„ [{i}/{len(selected_images)}] 3D ë³€í™˜ ì¤‘:")

                # HunYuan3D 3D ë³€í™˜ ì‹œë„
                success = self.run_hunyuan3d(selected_image, output_path)

                if success:
                    success_count += 1

                    # íŒŒì¼ í¬ê¸° í™•ì¸
                    model_size = (
                        os.path.getsize(output_path)
                        if os.path.exists(output_path)
                        else 0
                    )

                    file_info = {
                        "output_file": output_filename,
                        "output_path": output_path,
                        "model_size": model_size,
                        "model_used": "ndreca/hunyuan3d-2",
                        "status": "success",
                    }
                    processed_files.append(output_path)
                    print(f"   ğŸ“Š 3D ëª¨ë¸ í¬ê¸°: {model_size:,} bytes")
                    print(f"   ğŸ’¾ ì €ì¥ë¨: {output_filename}")

                else:
                    file_info = {
                        "output_file": output_filename,
                        "output_path": output_path,
                        "model_used": "ndreca/hunyuan3d-2",
                        "status": "failed",
                    }
                    processed_files.append(file_info)
                    print(f"   âŒ 3D ë³€í™˜ ì‹¤íŒ¨")

                # API í˜¸ì¶œ ê°„ ì ì‹œ ëŒ€ê¸° (Rate Limit ë°©ì§€)
                if i < len(selected_images):
                    print("   â³ 5ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(5)  # 3D ë³€í™˜ì€ ë” ë§ì€ ì‹œê°„ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ

            print("\n" + "=" * 60)
            print(f"ğŸ‰ 3D ë³€í™˜ ì‘ì—… ì™„ë£Œ!")
            print(f"âœ… ì„±ê³µ: {success_count}/{len(selected_images)}ê°œ")
            print(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {output_dir}/")
            print("=" * 60)

            return processed_files

        except Exception as e:
            print(f"âŒ 3D ë³€í™˜ ì‘ì—… ì˜¤ë¥˜: {e}")
            return []
